{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGSISGsDOPFYjqhEV21HTu"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kTV70oW3heq4"
      },
      "outputs": [],
      "source": [
        "#abc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import Omniglot\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "bFuWKqcChjiB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = 32\n",
        "\n",
        "\n",
        "train_set = Omniglot(\n",
        "    root=\"./data\",\n",
        "    background=True,\n",
        "    transform=transforms.Compose(\n",
        "        [\n",
        "            transforms.Grayscale(num_output_channels=3),\n",
        "            transforms.RandomResizedCrop(image_size),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "        ]\n",
        "    ),\n",
        "    download=True,\n",
        ")\n",
        "test_set = Omniglot(\n",
        "    root=\"./data\",\n",
        "    background=False,\n",
        "    transform=transforms.Compose(\n",
        "        [\n",
        "            # Omniglot images have 1 channel, but our model will expect 3-channel images\n",
        "            transforms.Grayscale(num_output_channels=3),\n",
        "            transforms.Resize([int(image_size * 1.15), int(image_size * 1.15)]),\n",
        "            transforms.CenterCrop(image_size),\n",
        "            transforms.ToTensor(),\n",
        "        ]\n",
        "    ),\n",
        "    download=True,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiYmUFDXhjk4",
        "outputId": "ffee2c12-e1e5-4d62-a139-7a7f507dd913"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "class PrototypicalNetworks(tf.keras.Model):\n",
        "    def __init__(self, backbone):\n",
        "        super(PrototypicalNetworks, self).__init__()\n",
        "        self.backbone = backbone\n",
        "\n",
        "    def call(\n",
        "        self,\n",
        "        support_images,\n",
        "        support_labels,\n",
        "        query_images,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Predict query labels using labeled support images.\n",
        "        \"\"\"\n",
        "        # Extract the features of support and query images\n",
        "        z_support = self.backbone(support_images, training=False)\n",
        "        z_query = self.backbone(query_images, training=False)\n",
        "\n",
        "        # Infer the number of different classes from the labels of the support set\n",
        "        n_way = tf.shape(tf.unique(support_labels)[0])[0]\n",
        "        # Prototype i is the mean of all instances of features corresponding to labels == i\n",
        "        z_proto = tf.concat(\n",
        "            [\n",
        "                tf.reduce_mean(tf.boolean_mask(z_support, support_labels == label), axis=0)\n",
        "                for label in tf.range(n_way)\n",
        "            ],\n",
        "            axis=0,\n",
        "        )\n",
        "\n",
        "        # Compute the euclidean distance from queries to prototypes\n",
        "        dists = tf.norm(tf.expand_dims(z_query, 1) - tf.expand_dims(z_proto, 0), axis=-1)\n",
        "\n",
        "        # And here is the super complicated operation to transform those distances into classification scores!\n",
        "        scores = -dists\n",
        "        return scores\n",
        "\n",
        "# Replace `resnet18` with the appropriate TensorFlow backbone model or implement it using TensorFlow if it doesn't exist.\n",
        "# The preprocessing might also differ for the TensorFlow backbone, depending on the model you use.\n",
        "# You can use a pre-trained model or a custom implementation.\n",
        "backbone_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "backbone_model = Flatten()((backbone_model.output))\n",
        "\n",
        "# Create the Prototypical Networks model with the TensorFlow backbone\n",
        "model = PrototypicalNetworks(backbone_model)\n"
      ],
      "metadata": {
        "id": "LpnV6XK7hjqM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "N_WAY = 5  # Number of classes in a task\n",
        "N_SHOT = 5  # Number of images per class in the support set\n",
        "N_QUERY = 10  # Number of images per class in the query set\n",
        "N_EVALUATION_TASKS = 100\n",
        "\n",
        "# Assuming 'test_set' contains the Omniglot dataset\n",
        "\n",
        "# Extract images and corresponding labels from the test_set\n",
        "test_images = [instance[0] for instance in test_set]\n",
        "test_labels = [instance[1] for instance in test_set]\n",
        "\n",
        "# Convert the lists to TensorFlow tensors\n",
        "test_images = tf.convert_to_tensor(test_images)\n",
        "test_labels = tf.convert_to_tensor(test_labels)\n",
        "\n",
        "# Create a task sampler for episodic evaluation\n",
        "def task_sampler(labels):\n",
        "    selected_labels = tf.random.shuffle(labels)[:N_WAY]\n",
        "    return selected_labels\n",
        "\n",
        "# Create a function to sample support and query sets for each task\n",
        "def sample_support_query(task_labels):\n",
        "    selected_classes = task_sampler(task_labels)\n",
        "    support_set_images = []\n",
        "    query_set_images = []\n",
        "\n",
        "    for class_label in selected_classes:\n",
        "        class_indices = tf.where(tf.equal(task_labels, class_label))[:, 0]\n",
        "        selected_indices = tf.random.shuffle(class_indices)[:N_SHOT + N_QUERY]\n",
        "        support_set_images.extend(test_images[selected_indices[:N_SHOT]])\n",
        "        query_set_images.extend(test_images[selected_indices[N_SHOT:]])\n",
        "\n",
        "    return support_set_images, query_set_images\n",
        "\n",
        "# Create support and query sets for each task in the evaluation\n",
        "test_support_images, test_query_images = [], []\n",
        "for _ in range(N_EVALUATION_TASKS):\n",
        "    support_set, query_set = sample_support_query(test_labels)\n",
        "    test_support_images.append(support_set)\n",
        "    test_query_images.append(query_set)\n",
        "\n",
        "# Convert the support and query sets to TensorFlow tensors\n",
        "test_support_images = tf.convert_to_tensor(test_support_images)\n",
        "test_query_images = tf.convert_to_tensor(test_query_images)\n",
        "\n",
        "# Create TensorFlow datasets for support and query sets\n",
        "test_support_loader = tf.data.Dataset.from_tensor_slices(test_support_images).batch(N_WAY * N_SHOT)\n",
        "test_query_loader = tf.data.Dataset.from_tensor_slices(test_query_images).batch(N_WAY * N_QUERY)\n",
        "\n",
        "# Prefetch the data to improve performance\n",
        "test_support_loader = test_support_loader.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "test_query_loader = test_query_loader.prefetch(buffer_size=tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "C51HiLnxhjsw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "outputId": "3b987915-bc09-4fd7-fad7-90ddead87580"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-f6aedc9fcedf>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Convert the lists to TensorFlow tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtest_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Attempt to convert a value (tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n         [1., 1., 1.,  ..., 1., 1., 1.],\n         [1., 1., 1.,  ..., 1., 1., 1.],\n         ...,\n         [1., 1., 1.,  ..., 1., 1., 1.],\n         [1., 1., 1.,  ..., 1., 1., 1.],\n         [1., 1., 1.,  ..., 1., 1., 1.]],\n\n        [[1., 1., 1.,  ..., 1., 1., 1.],\n         [1., 1., 1.,  ..., 1., 1., 1.],\n         [1., 1., 1.,  ..., 1., 1., 1.],\n         ...,\n         [1., 1., 1.,  ..., 1., 1., 1.],\n         [1., 1., 1.,  ..., 1., 1., 1.],\n         [1., 1., 1.,  ..., 1., 1., 1.]],\n\n        [[1., 1., 1.,  ..., 1., 1., 1.],\n         [1., 1., 1.,  ..., 1., 1., 1.],\n         [1., 1., 1.,  ..., 1., 1., 1.],\n         ...,\n         [1., 1., 1.,  ..., 1., 1., 1.],\n         [1., 1., 1.,  ..., 1., 1., 1.],\n         [1., 1., 1.,  ..., 1., 1., 1.]]])) with an unsupported type (<class 'torch.Tensor'>) to a Tensor."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qJAkNyHnhjvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xusZgLvRhjyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A_jWKqSjhj03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ySquWia0hj4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_p0At-m-hj7C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}